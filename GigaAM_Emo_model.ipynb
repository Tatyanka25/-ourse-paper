{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWs3Q8jhtFeFU067vdXWNX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tatyanka25/Course-paper/blob/main/GigaAM_Emo_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzbdn7e3B_j5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install matplotlib>=3.3.2\n",
        "\n",
        "BRANCH = 'r1.21.0'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "\n",
        "!pip install hydra-core\n",
        "!pip install omegaconf\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from omegaconf import DictConfig, ListConfig\n",
        "import hydra\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "!ls /content/gdrive/"
      ],
      "metadata": {
        "id": "lkQLAZPrCE85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpecScaler(torch.nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.log(x.clamp_(1e-9, 1e9))\n",
        "\n",
        "\n",
        "class GigaAMEmo(torch.nn.Module):\n",
        "    def __init__(self, conf: Union[DictConfig, ListConfig]):\n",
        "        super().__init__()\n",
        "        self.id2name = conf.id2name\n",
        "        self.feature_extractor = hydra.utils.instantiate(conf.feature_extractor)\n",
        "        self.conformer = hydra.utils.instantiate(conf.encoder)\n",
        "        self.linear_head = hydra.utils.instantiate(conf.classification_head)\n",
        "\n",
        "    def forward(self, features, features_length=None):\n",
        "        if features.dim() == 2:\n",
        "            features = features.unsqueeze(0)\n",
        "        if not features_length:\n",
        "            features_length = torch.ones(features.shape[0]) * features.shape[-1]\n",
        "            features_length = features_length.to(features.device)\n",
        "        encoded, _ = self.conformer(audio_signal=features, length=features_length)\n",
        "        encoded_pooled = torch.nn.functional.avg_pool1d(\n",
        "            encoded, kernel_size=encoded.shape[-1]\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        logits = self.linear_head(encoded_pooled)\n",
        "        return logits\n",
        "\n",
        "    def get_probs(self, audio_path: str) -> List[List[float]]:\n",
        "        audio_signal, _ = sf.read(audio_path, dtype=\"float32\")\n",
        "        features = self.feature_extractor(torch.tensor(audio_signal).float().to(next(self.parameters()).device))\n",
        "        logits = self.forward(features)\n",
        "        probs = torch.nn.functional.softmax(logits).detach().tolist()\n",
        "        return probs"
      ],
      "metadata": {
        "id": "cqkcxAFCCKQG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# Loading weights, config and example wav for CTC-model\n",
        "!wget https://n-ws-q0bez.s3pd12.sbercloud.ru/b-ws-q0bez-jpv/GigaAM/emo_model_weights.ckpt\n",
        "!wget https://n-ws-q0bez.s3pd12.sbercloud.ru/b-ws-q0bez-jpv/GigaAM/emo_model_config.yaml"
      ],
      "metadata": {
        "id": "ozHuR9VTCQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Мultiple emotions with probabilities**"
      ],
      "metadata": {
        "id": "RqtSSkbNtUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from omegaconf import OmegaConf\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "audio_path = '/content/gdrive/My Drive/Emotion_models/My_audio/angry.wav'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    probs = model.get_probs(audio_path)[0]\n",
        "print(\", \".join([f\"{model.id2name[i]}: {p:.3f}\" for i, p in enumerate(probs)]))"
      ],
      "metadata": {
        "id": "F0Gteg9OCZ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The most likely emotion**"
      ],
      "metadata": {
        "id": "gfcfAijhtZKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from omegaconf import OmegaConf\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "audio_path = '/content/gdrive/My Drive/Emotion_models/My_audio/angry.wav'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    probs = model.get_probs(audio_path)[0]\n",
        "emotion = max(enumerate(probs), key=lambda item: item[1])\n",
        "print(f\"Predicted emotion: {model.id2name[emotion[0]]}\")"
      ],
      "metadata": {
        "id": "jjx9BWGsNJWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RAVDESS**"
      ],
      "metadata": {
        "id": "7H4BlM7HRrsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RAVDESS\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "emotion_labels = {\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'positive',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/Emotions/Actor_*//*.wav\"):\n",
        "  audio_path=os.path.basename(file)\n",
        "  emotion = emotion_labels[audio_path.split(\"-\")[2]]\n",
        "  if emotion not in focused_emotion_labels:\n",
        "            continue\n",
        "  true_labels.append(emotion)\n",
        "  with torch.no_grad():\n",
        "    probs = model.get_probs(file)[0]\n",
        "  emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "  predictions.append(model.id2name[emotion_1[0]])\n",
        "  #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "accuracy = accuracy_score(y_true=true_labels, y_pred= predictions)\n",
        "print(\"Accuracy of the Recognizer is: {:.1f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "id": "Xf-Wat5ZJzI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RAVDESS: recognition accuracy for each emotion**"
      ],
      "metadata": {
        "id": "5jMGHNmgCax-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "emotion_labels = {\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'positive',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/Emotions/Actor_*//*.wav\"):\n",
        "  audio_path=os.path.basename(file)\n",
        "  emotion = emotion_labels[audio_path.split(\"-\")[2]]\n",
        "  if emotion not in focused_emotion_labels:\n",
        "            continue\n",
        "  true_labels.append(emotion)\n",
        "  with torch.no_grad():\n",
        "    probs = model.get_probs(file)[0]\n",
        "  emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "  predictions.append(model.id2name[emotion_1[0]])\n",
        "  #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "report = classification_report(y_true=true_labels, y_pred=predictions, target_names=focused_emotion_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "3MwGn5tbCaak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SAVEE**"
      ],
      "metadata": {
        "id": "Cf_UedkeRmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVEE\n",
        "import os, glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "emotion_labels = {\n",
        "  'n':'neutral',\n",
        "  'c':'calm',\n",
        "  'h':'positive',\n",
        "  'sa':'sad',\n",
        "  'a':'angry',\n",
        "  'f':'fearful',\n",
        "  'd':'disgust',\n",
        "  'su':'surprised'\n",
        "}\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/savee/*//*.wav\"):\n",
        "  audio_path=os.path.basename(file)\n",
        "  emotion = emotion_labels[re.search(r\"([a-zA-Z]+)\", audio_path).group(1)]\n",
        "  if emotion not in focused_emotion_labels:\n",
        "            continue\n",
        "  true_labels.append(emotion)\n",
        "  with torch.no_grad():\n",
        "    probs = model.get_probs(file)[0]\n",
        "  emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "  predictions.append(model.id2name[emotion_1[0]])\n",
        "  #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "accuracy = accuracy_score(y_true=true_labels, y_pred= predictions)\n",
        "print(\"Accuracy of the Recognizer is: {:.1f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "id": "vjNWtOauRFTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SAVEE: recognition accuracy for each emotion**"
      ],
      "metadata": {
        "id": "248mIMEdnUQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "\n",
        "emotion_labels = {\n",
        "  'n':'neutral',\n",
        "  'c':'calm',\n",
        "  'h':'positive',\n",
        "  'sa':'sad',\n",
        "  'a':'angry',\n",
        "  'f':'fearful',\n",
        "  'd':'disgust',\n",
        "  'su':'surprised'\n",
        "}\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/savee/*//*.wav\"):\n",
        "  audio_path=os.path.basename(file)\n",
        "  emotion = emotion_labels[re.search(r\"([a-zA-Z]+)\", audio_path).group(1)]\n",
        "  if emotion not in focused_emotion_labels:\n",
        "            continue\n",
        "  true_labels.append(emotion)\n",
        "  with torch.no_grad():\n",
        "    probs = model.get_probs(file)[0]\n",
        "  emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "  predictions.append(model.id2name[emotion_1[0]])\n",
        "  #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "report = classification_report(y_true=true_labels, y_pred=predictions, target_names=focused_emotion_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "fnG5jwPCnD7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DUSHA**"
      ],
      "metadata": {
        "id": "J-4vWt5uRdoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DUSHA\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Основы программирования/raw_crowd_test.tsv', sep='\\t')\n",
        "unique_emotions = df['speaker_emo'].unique()\n",
        "print(unique_emotions)"
      ],
      "metadata": {
        "id": "9qqJsilC20ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "# Чтение файла с информацией о данных\n",
        "info_df = pd.read_csv('/content/gdrive/My Drive/Основы программирования/raw_crowd_test.tsv', sep='\\t')\n",
        "\n",
        "# Перебор аудиофайлов\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/wavs_test/*.wav\"):\n",
        "    audio_path=os.path.basename(file)\n",
        "    audio_file_name = audio_path.split('.')[0]\n",
        "    # Проверка наличия названия аудиофайла в файле с информацией о данных\n",
        "    if audio_file_name in info_df['hash_id'].values:\n",
        "        if pd.isnull(info_df[info_df['hash_id'] == audio_file_name]['speaker_emo'].values[0]):\n",
        "            # Если столбец 'speaker_emo' пуст, продолжить\n",
        "            continue\n",
        "        else:\n",
        "            # В противном случае получить эмоцию, соответствующую аудиофайлу\n",
        "            emotion = info_df[info_df['hash_id'] == audio_file_name]['speaker_emo'].values[0]\n",
        "            true_labels.append(emotion)\n",
        "            with torch.no_grad():\n",
        "              probs = model.get_probs(file)[0]\n",
        "            emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "            predictions.append(model.id2name[emotion_1[0]])\n",
        "            #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "accuracy = accuracy_score(y_true=true_labels, y_pred= predictions)\n",
        "print(\"Accuracy of the Recognizer is: {:.1f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "id": "eYKkeTPZ3fQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DUSHA: recognition accuracy for each emotion**"
      ],
      "metadata": {
        "id": "V3VP-J6HtdsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "# Чтение файла с информацией о данных\n",
        "info_df = pd.read_csv('/content/gdrive/My Drive/Основы программирования/raw_crowd_test.tsv', sep='\\t')\n",
        "\n",
        "# Перебор аудиофайлов\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/wavs_test/*.wav\"):\n",
        "    audio_path=os.path.basename(file)\n",
        "    audio_file_name = audio_path.split('.')[0]\n",
        "    # Проверка наличия названия аудиофайла в файле с информацией о данных\n",
        "    if audio_file_name in info_df['hash_id'].values:\n",
        "        if pd.isnull(info_df[info_df['hash_id'] == audio_file_name]['speaker_emo'].values[0]):\n",
        "            # Если столбец 'speaker_emo' пуст, продолжить\n",
        "            continue\n",
        "        else:\n",
        "            # В противном случае получить эмоцию, соответствующую аудиофайлу\n",
        "            emotion = info_df[info_df['hash_id'] == audio_file_name]['speaker_emo'].values[0]\n",
        "            true_labels.append(emotion)\n",
        "            with torch.no_grad():\n",
        "              probs = model.get_probs(file)[0]\n",
        "            emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "            predictions.append(model.id2name[emotion_1[0]])\n",
        "            #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "report = classification_report(y_true=true_labels, y_pred=predictions, target_names=focused_emotion_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "G7V0cPdstdaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RESD**"
      ],
      "metadata": {
        "id": "6AkSm4O-RYtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RESD\n",
        "import os, glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "emotion_labels = {\n",
        "  'sadness':'sad',\n",
        "  'neutral':'neutral',\n",
        "  'happiness':'positive',\n",
        "  'anger':'angry',\n",
        "  'fear':'fearful',\n",
        "  'disgust':'disgust',\n",
        "  'enthusiasm':'enthusiasm'\n",
        "}\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "# Чтение файла с информацией о данных\n",
        "info_resd_df = pd.read_csv('/content/gdrive/My Drive/Основы программирования/resd_test.csv', sep=',')\n",
        "\n",
        "# Перебор аудиофайлов\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/resd_test/*//*.wav\"):\n",
        "    audio_path=os.path.basename(file)\n",
        "    audio_file_name = audio_path.split('.')[0]\n",
        "    # Проверка наличия названия аудиофайла в файле с информацией о данных\n",
        "    if audio_file_name in info_resd_df['name'].values:\n",
        "        if pd.isnull(info_resd_df[info_resd_df['name'] == audio_file_name]['emotion'].values[0]):\n",
        "            # Если столбец 'speaker_emo' пуст, продолжить\n",
        "            continue\n",
        "        else:\n",
        "            # В противном случае получить эмоцию, соответствующую аудиофайлу\n",
        "            emotion = emotion_labels[info_resd_df[info_resd_df['name'] == audio_file_name]['emotion'].values[0]]\n",
        "            if emotion not in focused_emotion_labels:\n",
        "              continue\n",
        "            true_labels.append(emotion)\n",
        "            with torch.no_grad():\n",
        "              probs = model.get_probs(file)[0]\n",
        "            emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "            predictions.append(model.id2name[emotion_1[0]])\n",
        "            #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "accuracy = accuracy_score(y_true=true_labels, y_pred= predictions)\n",
        "print(\"Accuracy of the Recognizer is: {:.1f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "id": "oC9g3sUSA6v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RESD: recognition accuracy for each emotion**"
      ],
      "metadata": {
        "id": "fZPO7mxuOf_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "from sklearn.metrics import classification_report\n",
        "import statistics\n",
        "\n",
        "\n",
        "emotion_labels = {\n",
        "  'sadness':'sad',\n",
        "  'neutral':'neutral',\n",
        "  'happiness':'positive',\n",
        "  'anger':'angry',\n",
        "  'fear':'fearful',\n",
        "  'disgust':'disgust',\n",
        "  'enthusiasm':'enthusiasm'\n",
        "}\n",
        "\n",
        "focused_emotion_labels = ['positive', 'sad', 'angry', 'neutral']\n",
        "\n",
        "model_config = 'emo_model_config.yaml'\n",
        "model_weights = 'emo_model_weights.ckpt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "conf = OmegaConf.load(model_config)\n",
        "model = GigaAMEmo(conf)\n",
        "ckpt = torch.load(model_weights, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt, strict=False)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "# Чтение файла с информацией о данных\n",
        "info_resd_df = pd.read_csv('/content/gdrive/My Drive/Основы программирования/resd_test.csv', sep=',')\n",
        "\n",
        "# Перебор аудиофайлов\n",
        "for file in glob.glob(\"/content/gdrive/My Drive/Основы программирования/resd_test/*//*.wav\"):\n",
        "    audio_path=os.path.basename(file)\n",
        "    audio_file_name = audio_path.split('.')[0]\n",
        "    # Проверка наличия названия аудиофайла в файле с информацией о данных\n",
        "    if audio_file_name in info_resd_df['name'].values:\n",
        "        if pd.isnull(info_resd_df[info_resd_df['name'] == audio_file_name]['emotion'].values[0]):\n",
        "            # Если столбец 'speaker_emo' пуст, продолжить\n",
        "            continue\n",
        "        else:\n",
        "            # В противном случае получить эмоцию, соответствующую аудиофайлу\n",
        "            emotion = emotion_labels[info_resd_df[info_resd_df['name'] == audio_file_name]['emotion'].values[0]]\n",
        "            if emotion not in focused_emotion_labels:\n",
        "              continue\n",
        "            true_labels.append(emotion)\n",
        "            with torch.no_grad():\n",
        "              probs = model.get_probs(file)[0]\n",
        "            emotion_1 = max(enumerate(probs), key=lambda item: item[1])\n",
        "            predictions.append(model.id2name[emotion_1[0]])\n",
        "            #print(f\"Predicted emotion: {model.id2name[emotion_1[0]]}\",\" Real emotion:\", emotion)\n",
        "\n",
        "report = classification_report(y_true=true_labels, y_pred=predictions, target_names=focused_emotion_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "WZPCme5oOgae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}